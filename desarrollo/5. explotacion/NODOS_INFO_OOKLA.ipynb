{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NODOS_INFO_OOKLA.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsVQAbZeWazE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "from joblib import parallel, delayed\n",
        "import math as m\n",
        "import datetime\n",
        "from seaborn import lmplot\n",
        "from seaborn import distplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHs6EKvfW87U"
      },
      "source": [
        "df_segmentados= pd.read_excel('/content/Segmentaciones_Nodos_Ejecutados.xlsx')\n",
        "df_segmentados.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQu5KwkOmJdh"
      },
      "source": [
        "df_segmentados.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q63oqGLIkmeQ"
      },
      "source": [
        "# # renombramos columnas\n",
        "# df_segmentados.rename(columns={'Nodo':'NODO'}, inplace=True)\n",
        "# df_segmentados.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmSsEhZWW8S8"
      },
      "source": [
        "df_Nodosokla= pd.read_csv('/content/NODOS_INFO_OOKLA.csv',sep=';', header = 0)\n",
        "df_Nodosokla.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZorxVxFW80m"
      },
      "source": [
        "df_Nodosokla.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_HBPDt9oX9V"
      },
      "source": [
        "df_Nodosokla['MES'] = [f'0{x}' if x <10 else str(x) for x in df_Nodosokla['MES']]\n",
        "df_Nodosokla['FECHA'] = df_Nodosokla['ANIO'].astype('str') + '-' + df_Nodosokla['MES']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_DDDAtDz1-R"
      },
      "source": [
        "df_Nodosokla = df_Nodosokla[['FECHA', 'NODO',\t'ANIO',\t'MES', 'OPERADOR', 'CANTIDAD',\t'DOWNLOAD_KBPS',\t'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT'\t]]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zs01srmoyFn"
      },
      "source": [
        "df_Nodosokla"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySdZ55ocLDaf"
      },
      "source": [
        "df_Nodosokla.to_csv('okla2.csv') # Carga de datos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opD1_GKoLYVt"
      },
      "source": [
        "# df_Nodosokla= pd.read_csv('/content/okla2.csv')\n",
        "# df_Nodosokla.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awwxl3Jm2LMw"
      },
      "source": [
        "nodos=df_Nodosokla[\"NODO\"].value_counts()\n",
        "nodos_con_mas_registros=nodos[:30]\n",
        "print(nodos_con_mas_registros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPdvf9yANIAF"
      },
      "source": [
        "InfoByNodo=df_Nodosokla[['NODO','FECHA']].groupby('NODO').count()\n",
        "InfoByNodo.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwEzk6t1NkF7"
      },
      "source": [
        "sampleDW=InfoByNodo[InfoByNodo['FECHA']==56].sort_values('FECHA',ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtbk2FJsNkwc"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "for ind, row in sampleDW.iterrows():   \n",
        "    df_Nodosokla[df_Nodosokla['NODO']==ind].sort_values('FECHA',ascending=True).plot(x='FECHA',y='DOWNLOAD_KBPS',ax=ax, kind='line', label=ind)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIFb-lBVlQs9"
      },
      "source": [
        "# # para unir csv por nodos pero hay que unir el nombre Nodo=NODO=NOD0 en ambos csv\n",
        "# clients1=pd.merge(df_Nodosokla, df_segmentados, on='NODO')\n",
        "# clients1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZE2JD-2BaXe"
      },
      "source": [
        "# essto me muestra que hay unna relacion lineal entre 'UPLOAD_KBPS', 'DOWNLOAD_KBPS'\n",
        "lmplot('UPLOAD_KBPS', 'DOWNLOAD_KBPS', data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_UHTl2-U8gi"
      },
      "source": [
        "from seaborn import lmplot\n",
        "lmplot(x=\"DOWNLOAD_KBPS\", y=\"UPLOAD_KBPS\", hue=\"OPERADOR\", data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SMsCUoZIBeI"
      },
      "source": [
        "#  tendencia de datos para claro\n",
        "lmplot(x=\"DOWNLOAD_KBPS\", y=\"UPLOAD_KBPS\", data=df_Nodosokla[df_Nodosokla['OPERADOR']=='CLARO'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zCUoEtBoCFJ"
      },
      "source": [
        "df_Nodosokla.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX870yHvDCHr"
      },
      "source": [
        "sns.catplot(y=\"CANTIDAD\", x=\"OPERADOR\", palette=\"dark\", data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aecdmiJo5jzu"
      },
      "source": [
        "sns.catplot(y=\"DOWNLOAD_KBPS\", x=\"OPERADOR\", kind=\"violin\", palette=\"dark\", data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T57tLW4roCQT"
      },
      "source": [
        "# Operadorcon mas registros en okla\n",
        "sns.countplot(x='OPERADOR', data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpWBqWdFKzV"
      },
      "source": [
        "# la latencia de red es la suma de retardos temporales dentro de una red. Un retardo es producido por la demora en la propagación y transmisión de paquetes dentro de la red. \n",
        "from seaborn import distplot\n",
        "distplot(df_Nodosokla.LATENCY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiEB4LGnw8e5"
      },
      "source": [
        "operadores=df_Nodosokla[\"OPERADOR\"].value_counts(ascending=False)#.tolist()\n",
        "print(operadores)\n",
        "print('suma de operadores registrados en okla',sum(operadores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1BiHMEeCk5h"
      },
      "source": [
        "veldescarga=df_Nodosokla[\"DOWNLOAD_KBPS\"].value_counts(ascending=False)#.tolist()\n",
        "veldescarga=veldescarga[:10]\n",
        "print(veldescarga)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNDw_NIzEDxj"
      },
      "source": [
        "velsubida=df_Nodosokla[\"UPLOAD_KBPS\"].value_counts(ascending=False)#.tolist()\n",
        "velsubida=velsubida[:10]\n",
        "print(velsubida)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PHNHOt33wic"
      },
      "source": [
        "df_Nodosokla.groupby(by='OPERADOR')['DOWNLOAD_KBPS','UPLOAD_KBPS','LATENCY',\t'FLAG_LAT'].max() # nos agrupa por operador y por cada tipo de funcion nos aplica la funcion maxima aplicado a cada operador"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U06Nn6q09cYl"
      },
      "source": [
        "df_Nodosokla.groupby(by='OPERADOR')['DOWNLOAD_KBPS','UPLOAD_KBPS','LATENCY',\t'FLAG_LAT'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGZZbSfv_bet"
      },
      "source": [
        "df_Nodosokla.groupby(by='OPERADOR')['DOWNLOAD_KBPS','UPLOAD_KBPS','LATENCY',\t'FLAG_LAT'].describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcMPQKHI9c1R"
      },
      "source": [
        "gb_df= pd.DataFrame(df_Nodosokla.groupby(['OPERADOR','UPLOAD_KBPS'],as_index=False)['LATENCY'].mean())\n",
        "gb_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3s6WzJtsZj5"
      },
      "source": [
        "from seaborn import boxplot\n",
        "boxplot(df_Nodosokla.DOWNLOAD_KBPS, orient=\"v\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxQc75xzoCVq"
      },
      "source": [
        "# sns.jointplot(x='FECHA',y='LATENCY',data=df_Nodosokla, kind='scatter',height=10, ratio=3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKAqGzxb19iy"
      },
      "source": [
        "sns.jointplot(x='UPLOAD_KBPS',y='LATENCY',data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55hfC_Yw4jLK"
      },
      "source": [
        "sns.jointplot(x='DOWNLOAD_KBPS',y='LATENCY',data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBm1yJzKoCbR"
      },
      "source": [
        "from seaborn import lmplot\n",
        "lmplot('DOWNLOAD_KBPS', 'LATENCY', data=df_Nodosokla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YVSj0KLoCgg"
      },
      "source": [
        "# distribución de la variable DOWNLOAD_KBPS\n",
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6, 6))\n",
        "sns.distplot(\n",
        "    df_Nodosokla.DOWNLOAD_KBPS,\n",
        "    hist    = False,\n",
        "    rug     = True,\n",
        "    color   = \"blue\",\n",
        "    kde_kws = {'shade': True, 'linewidth': 1},\n",
        "    ax      = axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Distribución original\", fontsize = 'medium')\n",
        "axes[0].set_xlabel('Download', fontsize='small') \n",
        "axes[0].tick_params(labelsize = 6)\n",
        "\n",
        "sns.distplot(\n",
        "    np.sqrt(df_Nodosokla.DOWNLOAD_KBPS),\n",
        "    hist    = False,\n",
        "    rug     = True,\n",
        "    color   = \"blue\",\n",
        "    kde_kws = {'shade': True, 'linewidth': 1},\n",
        "    ax      = axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Transformación raíz cuadrada\", fontsize = 'medium')\n",
        "axes[1].set_xlabel('sqrt(Download)', fontsize='small') \n",
        "axes[1].tick_params(labelsize = 6)\n",
        "\n",
        "sns.distplot(\n",
        "    np.log(df_Nodosokla.DOWNLOAD_KBPS),\n",
        "    hist    = False,\n",
        "    rug     = True,\n",
        "    color   = \"blue\",\n",
        "    kde_kws = {'shade': True, 'linewidth': 1},\n",
        "    ax      = axes[2]\n",
        ")\n",
        "axes[2].set_title(\"Transformación logarítmica\", fontsize = 'medium')\n",
        "axes[2].set_xlabel('log(Download)', fontsize='small') \n",
        "axes[2].tick_params(labelsize = 6)\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lu3k8XiRWYQ"
      },
      "source": [
        "# distribución de la variable UPLOAD_KBPS\n",
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6, 6))\n",
        "sns.distplot(\n",
        "    df_Nodosokla.UPLOAD_KBPS,\n",
        "    hist    = False,\n",
        "    rug     = True,\n",
        "    color   = \"blue\",\n",
        "    kde_kws = {'shade': True, 'linewidth': 1},\n",
        "    ax      = axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Distribución original\", fontsize = 'medium')\n",
        "axes[0].set_xlabel('Upload', fontsize='small') \n",
        "axes[0].tick_params(labelsize = 6)\n",
        "\n",
        "sns.distplot(\n",
        "    np.sqrt(df_Nodosokla.UPLOAD_KBPS),\n",
        "    hist    = False,\n",
        "    rug     = True,\n",
        "    color   = \"blue\",\n",
        "    kde_kws = {'shade': True, 'linewidth': 1},\n",
        "    ax      = axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Transformación raíz cuadrada\", fontsize = 'medium')\n",
        "axes[1].set_xlabel('sqrt(Upload)', fontsize='small') \n",
        "axes[1].tick_params(labelsize = 6)\n",
        "\n",
        "sns.distplot(\n",
        "    np.log(df_Nodosokla.UPLOAD_KBPS),\n",
        "    hist    = False,\n",
        "    rug     = True,\n",
        "    color   = \"blue\",\n",
        "    kde_kws = {'shade': True, 'linewidth': 1},\n",
        "    ax      = axes[2]\n",
        ")\n",
        "axes[2].set_title(\"Transformación logarítmica\", fontsize = 'medium')\n",
        "axes[2].set_xlabel('log(Upload)', fontsize='small') \n",
        "axes[2].tick_params(labelsize = 6)\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp46ztYloCls"
      },
      "source": [
        "x = df_Nodosokla['CANTIDAD'].values # returns # CANTIDAD\tDOWNLOAD_KBPS\tUPLOAD_KBPS\tLATENCY\tFLAG_LAT\n",
        "x_str='Real returns'  # label e,g ric\n",
        "x_size= len(x)  #  retorna el tamaño df_Nodosokla['CANTIDAD'].shape\n",
        "print(x_size)\n",
        "print(df_Nodosokla.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbzf_7RCZTY0"
      },
      "source": [
        "# round_digits = 4\n",
        "from scipy.stats import skew, kurtosis, chi2\n",
        "\n",
        "x_mean = np.mean(x) # para calcular la  media\n",
        "x_std =np.std(x) # para la desviacion estandar o volatilidad si se habla de mercados\n",
        "x_skew = skew(x) # para calcular la skweness o simetria\n",
        "x_kurt = kurtosis(x) # curtosis en exceso, para calcular la kurtosis y ver que tantas colas largas tiene\n",
        "x_sharpe = x_mean/x_std # esto es el rendimiento en unidades de riesgo\n",
        "x_var_95 = np.percentile(x,95) # valor en riesgo esto nos dice que solamente el 5% de los valores son mayores, el valor arrojado en este caso mayores a 1.64\n",
        "x_cvar_95 = np.mean(x[x <= x_var_95]) # valor en riesgo condicional, esto nos dice, que cuando pierdo que tanto pierdo, esto es el promedio de todos los valores que estan a la isquierda osea que se encuentran en el -5% en este caso\n",
        "x_sharpe = x_mean / x_std * np.sqrt(252) # indica el rendimiento esperado por unidad de riesgo 252 eselnumero de dias anuales trabajados\n",
        "x_corrcoef = np.corrcoef(x) # coeficiente de correlacion\n",
        "x_var = np.var(x) # covarianza\n",
        "# Test de jarque-bera\n",
        "jb = x_size/6*(x_skew**2 + 1/4*x_kurt**2) # jarque bera\n",
        "# p-value\n",
        "p_value = 1 - chi2.cdf(jb, df=2) # se deben fijar df en 2 para la t-student, df= grados de liberdad \n",
        "is_normal = (p_value > 0.05) # con esto necesitamos que equivalently jb < 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1JnLNmzZTfM"
      },
      "source": [
        "print('media               =', x_mean)\n",
        "print('desviacion estandar =', x_std) # indica qué tan dispersos están los datos con respecto a la media para la desviacion estandar o volatilidad si se habla de mercado\n",
        "print('skweness            =', x_skew) # skweness es una variable aleatoria\n",
        "print('kurtosis            =', x_kurt) # curtosis en exceso, determina el grado de concentración que presentan los valores de una variable alrededor de la zona central de la distribución de frecuencias\n",
        "print('x_sharpe            =', x_sharpe)\n",
        "print('valor en riesgo     =', x_var_95) # Valor en riesgo esto nos dice que solamente el 5% de los valores son mayores, el valor arrojado en este caso mayores\n",
        "print('riesgo condicional  =', x_cvar_95)\n",
        "print('x_sharpe            =', x_sharpe)\n",
        "print('Coefic correlacion  =', x_corrcoef)\n",
        "print('varianza            =', x_var)\n",
        "print('jarque bera         =', jb) # si se tiene un jarque bera > 6 la distribucion no es normal\n",
        "print('p_value             =', p_value) # p < 0,05 significa que la hipótesis nula es falsa y una p > 0,05 que la hipótesis nula es verdadera: # esto me da la probabilidad de que suponiendo que tenga una distribicion chi2, cual seria la probabilidad # de tener puntos que caigan a la izquierda de dicho valor\n",
        "print('is_normal           =', is_normal) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s621LqYUPvdH"
      },
      "source": [
        "# para ver el grado de correlación entre las variables disponibles.\n",
        "def tidy_corr_matrix(corr_mat):\n",
        "    '''\n",
        "    Función para convertir una matrix de correlación de pandas en formato tidy\n",
        "    '''\n",
        "    corr_mat = corr_mat.stack().reset_index()\n",
        "    corr_mat.columns = ['variable_1','variable_2','r']\n",
        "    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n",
        "    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n",
        "    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n",
        "    \n",
        "    return(corr_mat)\n",
        "\n",
        "\n",
        "\n",
        "corr_matrix = df_Nodosokla.select_dtypes(include=['float64', 'int']).corr(method='pearson')\n",
        "tidy_corr_matrix(corr_matrix).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSaZsUZ3P9Iv"
      },
      "source": [
        "# Heatmap matriz de correlaciones\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(22, 12))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot     = True,\n",
        "    cbar      = False,\n",
        "    annot_kws = {\"size\": 1},\n",
        "    vmin      = -1,\n",
        "    vmax      = 1,\n",
        "    center    = 0,\n",
        "    cmap      = sns.diverging_palette(20, 220, n=200),\n",
        "    square    = True,\n",
        "    ax        = ax\n",
        ")\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation = 45,\n",
        "    horizontalalignment = 'right',\n",
        ")\n",
        "\n",
        "ax.tick_params(labelsize = 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbAQcAlg9qbx"
      },
      "source": [
        "# # modelo xgboost\n",
        "# import xgboost as xgb\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import RepeatedKFold\n",
        "# from xgboost import XGBRegressor\n",
        "# from xgboost import XGBRFClassifier, XGBClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.linear_model import LogisticRegression # clasificador linear\n",
        "# from sklearn.metrics import confusion_matrix # es una manera para resumir los datos \n",
        "\n",
        "# from sklearn.metrics import mean_squared_error as mse\n",
        "# from sklearn.datasets import load_boston\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from yellowbrick.regressor import residuals_plot\n",
        "# from yellowbrick.regressor import prediction_error\n",
        "# from sklearn.metrics import r2_score\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from scipy.stats import chi2_contingency\n",
        "# from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "# import statsmodels.formula.api as smf\n",
        "# import statsmodels.api as sm\n",
        "# from sklearn.metrics  import r2_score, mean_squared_error # metrica para medir\n",
        "\n",
        "\n",
        "# cols_entrenamiento= ['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT']\n",
        "\n",
        "\n",
        "# X = df_Nodosokla[cols_entrenamiento] #.values.reshape(-1, 1)\n",
        "# y= df_Nodosokla['DOWNLOAD_KBPS'] # score es una columna, esto quiere decir que de acuerdo a x determinamos y\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=22) \n",
        "\n",
        "\n",
        "# xgb_reg = xgb.XGBRegressor(max_depth=3, n_estimators=100, n_jobs=2,\n",
        "#                            objective='reg:squarederror', \n",
        "#                            booster='gbtree',\n",
        "#                            random_state=42, learning_rate=0.05)\n",
        "\n",
        "# # Train the model with train data sets\n",
        "# xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# # x_p=dfB[cols_prediccion]\n",
        "\n",
        "# y_pred = xgb_reg.predict(X_test) # Predictions\n",
        "# y_true = y_test # True values\n",
        "\n",
        "# # y_pred = xgb_reg.predict(X)\n",
        "# # y_pred_test = xgb_reg.predict(x_test)\n",
        "\n",
        "# MSE = mse(y_true, y_pred)\n",
        "# RMSE = np.sqrt(MSE)\n",
        "\n",
        "# R_squared = r2_score(y_test, y_pred)\n",
        "# print(y_train.shape)\n",
        "# print(y_test.shape)\n",
        "# print(y_pred.shape)\n",
        "# print(\"\\nRMSE: \", np.round(RMSE, 2))\n",
        "# print('y_pred',y_pred.shape)\n",
        "# print(\"R-Squared: \", np.round(R_squared, 2))\n",
        "# print('exactitud ',accuracy_score)\n",
        "\n",
        "# predicions=pd.DataFrame(y_pred,columns=['DOWNLOAD_KBPS'])\n",
        "# print(predicions)\n",
        "# print(predicions.shape)\n",
        "# # # predicions.to_csv('valor_total_avaluo.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1HWDJAQs9TT"
      },
      "source": [
        "# import xgboost as xgb\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import RepeatedKFold\n",
        "# from xgboost import XGBRegressor\n",
        "# from xgboost import XGBRFClassifier, XGBClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.linear_model import LogisticRegression # clasificador linear\n",
        "# from sklearn.metrics import confusion_matrix # es una manera para resumir los datos \n",
        "# from sklearn import tree\n",
        "# from sklearn.metrics import mean_squared_error as mse\n",
        "# from sklearn.datasets import load_boston\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from yellowbrick.regressor import residuals_plot\n",
        "# from yellowbrick.regressor import prediction_error\n",
        "# from sklearn.metrics import r2_score\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from scipy.stats import chi2_contingency\n",
        "# from sklearn.covariance import EllipticEnvelope\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "# from sklearn.datasets import make_classification\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "# import statsmodels.formula.api as smf\n",
        "# import statsmodels.api as sm\n",
        "# from sklearn.metrics  import r2_score, mean_squared_error # metrica para medir\n",
        "# from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "\n",
        "# clf = DecisionTreeRegressor(criterion='mse',max_depth=100, random_state=0)\n",
        "\n",
        "# x = df_Nodosokla[['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT']]\n",
        "# # x_test = df_Nodosokla[['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT']]\n",
        "# y = df_Nodosokla['DOWNLOAD_KBPS'] \n",
        "# # y_test = df_Nodosokla['DOWNLOAD_KBPS'] \n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=22) \n",
        "\n",
        "# RMSE = np.sqrt(MSE)\n",
        "\n",
        "# clf = clf.fit(X_train, y_train)\n",
        "# y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# # print(' R_cuadrado ', r2_score(y_test, y_pred))\n",
        "# print(\"\\nRMSE: \", np.round(RMSE, 2))\n",
        "# print(\"R-Squared: \", np.round(R_squared, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNy9qA6j7gbX"
      },
      "source": [
        "# # modelo de Decision Tree Regressor\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.linear_model import LogisticRegression # clasificador linear\n",
        "# from sklearn.datasets import make_classification\n",
        "# from sklearn.preprocessing import StandardScaler # StandardScaler sive para normalizar nuestros datos osea  para que esten en la misma escala\n",
        "# from sklearn.model_selection import train_test_split # para poder partir nuestros datos\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# cols_entrenamiento= ['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT']\n",
        "\n",
        "# X = df_Nodosokla[cols_entrenamiento]\n",
        "# y= df_Nodosokla['DOWNLOAD_KBPS']\n",
        "# # En nuestros features tendremos definidos 155 registros, uno por cada pais, 7 columnas 1 por cada pais \n",
        "# print('X.shape',X.shape)\n",
        "# # Y 155 para nuestra columna para nuestro target \n",
        "# print('y.shape',y.shape)\n",
        "# # tendremos 25% de nuestros datos para test y 75% para train \n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=22) \n",
        "\n",
        "# dt_reg=DecisionTreeRegressor(criterion='mse',max_depth=100, random_state=0)\n",
        "\n",
        "\n",
        "# dt_reg.fit(X_train,y_train)\n",
        "# y_pred = dt_reg.predict(X_test)\n",
        "# accuracy_score = dt_reg.score(X_test,y_test)\n",
        "\n",
        "# # y_pred = dt_reg.predict(X_test)\n",
        "\n",
        "\n",
        "# MSE = mse(y_true, y_pred)\n",
        "# RMSE = np.sqrt(MSE)\n",
        "\n",
        "# R_squared = r2_score(y_test, y_pred)\n",
        "# print('y_train',y_train.shape)\n",
        "# print('y_test',y_test.shape)\n",
        "# print(\"\\nRMSE: \", np.round(RMSE, 2))\n",
        "# print('y_pred',y_pred.shape)\n",
        "\n",
        "# print(\"R-Squared: \", np.round(R_squared, 2)) # RMSE indican un mejor ajuste.\n",
        "# print('exactitud ',accuracy_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKvzAEfaJnVJ"
      },
      "source": [
        "# # modelo de regrecion linear\n",
        "# from sklearn import datasets, linear_model # este ultimo para ajustar el modelo\n",
        "# from sklearn.metrics import r2_score\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.linear_model import Lasso\n",
        "# from sklearn.linear_model import Ridge\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib\n",
        "\n",
        "# # La función sample() devuelve de una lista de elementos un determinado número de elementos diferentes elegidos al azar.\n",
        "# np.random.seed(0) # semilla\n",
        "\n",
        "\n",
        "# cols_entrenamiento= ['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT']\n",
        "\n",
        "# X = df_Nodosokla[cols_entrenamiento] #.values.reshape(-1, 1)\n",
        "# y= df_Nodosokla['DOWNLOAD_KBPS'] # score es una columna, esto quiere decir que de acuerdo a x determinamos y\n",
        "\n",
        "# # tendremos 25% de nuestros datos para test y 75% para train \n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=22, shuffle= True) \n",
        "\n",
        "\n",
        "# # Creación del modelo\n",
        "# # ==============================================================================\n",
        "# modelo = LinearRegression()\n",
        "# modelo.fit(X = X_train, y = y_train)\n",
        "\n",
        "# print(\"Intercept:\", modelo.intercept_)\n",
        "# print(\"Coeficiente:\", list(zip(X.columns, modelo.coef_.flatten(), )))\n",
        "# print(\"Coeficiente de determinación R^2:\", modelo.score(X, y))\n",
        "\n",
        "# # Error de test del modelo \n",
        "# # ==============================================================================\n",
        "# predicciones = modelo.predict(X = X_test)\n",
        "# print(predicciones[0:3,])\n",
        "\n",
        "# rmse = mean_squared_error(\n",
        "#         y_true  = y_test,\n",
        "#         y_pred  = predicciones,\n",
        "#         squared = False\n",
        "#        )\n",
        "# print(\"\")\n",
        "# print(f\"El error (rmse) de test es: {rmse}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GD3kekKtApr"
      },
      "source": [
        "# Modelos unificados\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import XGBRFClassifier, XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression # clasificador linear\n",
        "from sklearn.metrics import confusion_matrix # es una manera para resumir los datos \n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from yellowbrick.regressor import residuals_plot\n",
        "from yellowbrick.regressor import prediction_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "from sklearn import datasets, linear_model # este ultimo para ajustar el modelo\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler # StandardScaler sive para normalizar nuestros datos osea  para que esten en la misma escala\n",
        "\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics  import r2_score, mean_squared_error # metrica para medir\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # La función sample() devuelve de una lista de elementos un determinado número de elementos diferentes elegidos al azar.\n",
        "  np.random.seed(0) # semilla\n",
        "\n",
        "  # Importamos el dataset del 2017 \n",
        "  # df_Nodosokla= pd.read_csv('/content/NODOS_INFO_OOKLA.csv',sep=';', header = 0)\n",
        "  # Mostramos el reporte estadistico\n",
        "  print('datos ',df_Nodosokla.describe())\n",
        "  df_Nodosokla= df_Nodosokla.sample(frac=1) # reoredenamos los datos\n",
        "\n",
        "\n",
        "  cols_entrenamiento= ['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT']\n",
        "\n",
        "  X = df_Nodosokla[cols_entrenamiento] #.values.reshape(-1, 1)\n",
        "  y= df_Nodosokla['DOWNLOAD_KBPS'] # score es una columna, esto quiere decir que de acuerdo a x determinamos y\n",
        "\n",
        "  # tendremos 25% de nuestros datos para test y 75% para train \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=22, shuffle= True) \n",
        "\n",
        "  estimadores = {\n",
        "      'modelxgboot' : xgb.XGBRegressor(max_depth=3, n_estimators=100, n_jobs=2, objective='reg:squarederror', booster='gbtree', random_state=42, learning_rate=0.05),\n",
        "      'modelDTree' : DecisionTreeRegressor(criterion='mse',max_depth=100, random_state=0),\n",
        "      'modelLinear' : LinearRegression(),\n",
        "      'modelLasso' : Lasso(alpha=0.2) , \n",
        "      'modelRidge' : Ridge(alpha=1)  \n",
        "    }\n",
        "  for name, estimator in estimadores.items():\n",
        "      modelo=estimator.fit(X_train, y_train) # ajustamos los datos de entrenamiento\n",
        "      predictions = estimator.predict(X_test)\n",
        "\n",
        "      MSE = mse(y_test, predictions) # el MSE es el error cuadrático medio de un estimador mide el promedio de los errores al cuadrado, es decir, la diferencia entre el estimador y lo que se estima. El ECM es una función de riesgo, correspondiente al valor esperado de la pérdida del error al cuadrado o pérdida cuadrática.\n",
        "      RMSE = np.sqrt(MSE)\n",
        "      R_squared = r2_score(y_test, predictions)\n",
        "\n",
        "      print(\"=\"*64) # estos nos imprimira 64 veces = antes de sacar nuestros datos\n",
        "      print(name)\n",
        "      print(\"MSE: \", mean_squared_error(y_test, predictions)) # imprimimos nuestro error pasandole\n",
        "      print(\"\\nRMSE: \", np.round(RMSE, 2))\n",
        "      print(\"R-Squared: \", np.round(R_squared, 2))\n",
        "      print('exactitud ',accuracy_score)\n",
        "\n",
        "        # nuestros datos de prueba y nuestras preciones\n",
        "        # nuestros datos de prueba y nuestras preciones\n",
        "  for name, estimator in estimadores.items():\n",
        "      print(\"=\"*32)\n",
        "      print(name)    \n",
        "      print(modelo.coef_)\n",
        "\n",
        "# R_squared = r2_score(y_test, y_pred)\n",
        "# print(y_train.shape)\n",
        "# print(y_test.shape)\n",
        "# print(y_pred.shape)\n",
        "\n",
        "# print('y_pred',y_pred.shape)\n",
        "# print(\"R-Squared: \", np.round(R_squared, 2))\n",
        "# print('exactitud ',accuracy_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBDyWGcJywj5"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "print(y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlYADFwWlAuR"
      },
      "source": [
        "# tratando de comparar variables\n",
        "# df_Nodosokla['NODO']=df_Nodosokla['NODO'].str.upper()\n",
        "# df_segmentados['Nodo']=df_segmentados['Nodo'].str.upper()\n",
        "\n",
        "# df=df_Nodosokla['Check']=df_segmentados.Nodo.isin(df_Nodosokla)\n",
        "# print(df_Nodosokla['Check'].value_counts())\n",
        "\n",
        "# print(df_Nodosokla.loc[df_Nodosokla.Check==True,'NODO'])\n",
        "\n",
        "# df_Nodosokla"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCmbeJ4tne6b"
      },
      "source": [
        "# df_shot=df_Nodosokla[df_Nodosokla[\"Check\"]=='True']\n",
        "# df_shot.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFRuDTron9Mu"
      },
      "source": [
        "# # renombramos columnas\n",
        "# df_segmentados.rename(columns={'Nodo':'NODO'}, inplace=True)\n",
        "# df_segmentados.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srx339D5n4Zd"
      },
      "source": [
        "# # para unir csv por nodos pero hay que unir el nombre Nodo=NODO=NOD0 en ambos csv\n",
        "# clients1=pd.merge(df_Nodosokla, df_segmentados, on='NODO')\n",
        "# clients1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCB3mQ-tpRNF"
      },
      "source": [
        "# df_Nodosokla.merge(df_segmentados, on=\"NODO\", how=\"left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEWXSFOfpvxl"
      },
      "source": [
        "# ccn=pd.concat([df_Nodosokla, df_segmentados], axis=1)\n",
        "# ccn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIyBTHw1svDy"
      },
      "source": [
        "# outerjoin=df_Nodosokla[df_Nodosokla['NODO']].merge(df_segmentados[df_segmentados['Nodo']], how ='outer', indicator=True)\n",
        "# print(outerjoin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJUsPLkSwAze"
      },
      "source": [
        "# # https://www.analyticslane.com/2018/09/10/unir-y-combinar-dataframes-con-pandas-en-python/\n",
        "# Newdf = pd.concat([df_Nodosokla, df_segmentados])\n",
        "# Newdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojT1vA6ewEKM"
      },
      "source": [
        "# df_sho=Newdf[Newdf[\"NODO\"]=='ILA']\n",
        "# df_sho.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgp1Z-IwSj4N"
      },
      "source": [
        "# CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT' 'DOWNLOAD_KBPS'\n",
        "df_Nodosokla.drop(['DOWNLOAD_KBPS', 'UPLOAD_KBPS'],1).hist()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8dxAQKLUF9e"
      },
      "source": [
        "# https://www.aprendemachinelearning.com/regresion-lineal-en-espanol-con-python/\n",
        "\n",
        "filtered_data = df_Nodosokla[(df_Nodosokla['DOWNLOAD_KBPS'] <= 100) & (df_Nodosokla['UPLOAD_KBPS'] >= 5)]\n",
        " \n",
        "colores=['orange','blue']\n",
        "tamanios=[30,60]\n",
        " \n",
        "f1 = filtered_data['DOWNLOAD_KBPS'].values\n",
        "f2 = filtered_data['UPLOAD_KBPS'].values\n",
        " \n",
        "# Vamos a pintar en colores los puntos por debajo y por encima de la media de Cantidad de Palabras\n",
        "asignar=[]\n",
        "for index, row in filtered_data.iterrows():\n",
        "    if(row['DOWNLOAD_KBPS']>100):\n",
        "        asignar.append(colores[0])\n",
        "    else:\n",
        "        asignar.append(colores[1])\n",
        "    \n",
        "plt.scatter(f1, f2, c=asignar, s=tamanios[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH_1ExDjV_d3"
      },
      "source": [
        "# https://relopezbriega.github.io/blog/2016/09/26/series-de-tiempo-con-python/\n",
        "df_Nodosokla.set_index('FECHA',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5IwPOV_WCtK"
      },
      "source": [
        "# df_Nodosokla= df_Nodosokla.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2OjPqT0WEF2"
      },
      "source": [
        "df_Nodosokla"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6pRExTEVYUb"
      },
      "source": [
        "# 'CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT'\n",
        "df_Nodosokla['CANTIDAD'].plot(linewidth=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfDAXLzaXeK3"
      },
      "source": [
        "# graficando Adj Close\n",
        "plot = df_Nodosokla['CANTIDAD'].plot(figsize=(10, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMo62c9EXrZ1"
      },
      "source": [
        "# Aplicando el filtro Hodrick-Prescott para separar en tendencia y \n",
        "# componente ciclico.\n",
        "wft_ciclo, wft_tend = sm.tsa.filters.hpfilter(df_Nodosokla['DOWNLOAD_KBPS'])\n",
        "df_Nodosokla['tend'] = wft_tend\n",
        "\n",
        "# graficando la variacion  precio real con la tendencia.\n",
        "df_Nodosokla[['DOWNLOAD_KBPS', 'tend']].plot(figsize=(10, 8), fontsize=12);\n",
        "legend = plt.legend()\n",
        "legend.prop.set_size(14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdxtrmfRWf9X"
      },
      "source": [
        "# calculando el porcentaje de variación de la semana\n",
        "variacion_diaria = df_Nodosokla['DOWNLOAD_KBPS'] / df_Nodosokla['DOWNLOAD_KBPS'].shift(1) - 1\n",
        "df_Nodosokla['var_semanal'] = variacion_diaria\n",
        "df_Nodosokla['var_semanal'][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_1GuGyNay75"
      },
      "source": [
        "plot = df_Nodosokla['var_semanal'].plot(figsize=(10, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_S3LfuFSOHy"
      },
      "source": [
        "df_Nodosokla=df_Nodosokla[[\t'NODO', 'ANIO',\t'MES','OPERADOR','CANTIDAD', 'DOWNLOAD_KBPS', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT', 'tend',\t'var_semanal'\t]].dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDVg3YQBTI6d"
      },
      "source": [
        "df_Nodosokla"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vlVE5kma3KM"
      },
      "source": [
        "# # modelo ARIMA sobre variación diaria\n",
        "# modelo = sm.tsa.ARIMA(df_Nodosokla['var_semanal'].iloc[1:], order=(1, 0, 0))  \n",
        "# resultados = modelo.fit(disp=-1)\n",
        "# df_Nodosokla['pronostico'] = resultados.fittedvalues  \n",
        "# plot = df_Nodosokla[['var_semanal', 'pronostico']].plot(figsize=(10, 8)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNbA3Mt7ejaJ"
      },
      "source": [
        "# para ver el Número de datos ausentes por variable\n",
        "df_Nodosokla.isnull().sum().sort_values(ascending=False,)[:21]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDTeWMW1XsFl"
      },
      "source": [
        "df_Nodosokla"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS1hy3KHQBRL"
      },
      "source": [
        "df_Nodosokla= df_Nodosokla.sample(frac=1) # reoredenamos los datos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NXE2fRmlUH4"
      },
      "source": [
        "# confusion_matrix=confusion_matrix(y_test, y_pred)\n",
        "# print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klyxrt-aeIgq"
      },
      "source": [
        "# from sklearn import tree\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier() # create objet\n",
        "\n",
        "# x = df_Nodosokla[['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT', 'DOWNLOAD_KBPS']]\n",
        "# x_test = df_Nodosokla[['CANTIDAD', 'UPLOAD_KBPS',\t'LATENCY',\t'FLAG_LAT', 'DOWNLOAD_KBPS']]\n",
        "# y = df_Nodosokla['NODO'] \n",
        "\n",
        "# clf = clf.fit(x,y) # Train objet\n",
        "# y_pred = clf.predict(x)\n",
        "# y_pred_test = clf.predict(x_test)\n",
        "\n",
        "# np.mean(y==y_pred) # comparamos que % se predice correctamente\n",
        "# # esta nos dice que el modelo predice muy bien la primera particion de los datos\n",
        "# np.mean(y_pred == y_pred_test) # este modelo  predice la particion del dftest\n",
        "# # y nos indica el % correctopredecido por este modelo\n",
        "# print(' R_cuadrado ', r2_score(y, y_pred))\n",
        "# print(' R_cuadrado ', r2_score(y_test, y_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}